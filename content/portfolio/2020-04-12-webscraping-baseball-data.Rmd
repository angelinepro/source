---
title: Webscraping Baseball Data using Python
author: Angeline Protacio
date: '2020-04-12'
weight: 1
slug: webscraping
output:
  blogdown::html_page:
    toc: true 
categories: []
tags:
  - Python
  - BeautifulSoup
  - Selenium
  - baseball
  - fantasy
  - ggplot2
  - Quality_Start
description: "An ephemeral method to collect baseball data when there isn't a relevant API or library with your data"
image: "img/portfolio/manarola.jpg"
---

### Why Scrape Baseball Data?

I recently enrolled in a data science bootcamp at Metis. Throughout the 12 weeks of the bootcamp, we work on five different projects to apply what we've learned thus far. One of the current projects includes webscraping data to use for predictive modeling. I opted to try my luck at scraping data from Fangraphs and Baseball Reference, neither of which have APIs that make it easy to get their data. As part of my desire to get better at fantasy baseball, I have also been exploring how to get historical projection data, which is available from Fangraphs, and is otherwise trickier to find using existing baseball Python and R libraries, so this was a good opportunity to source that data.

I wrote this post to detail my methods for getting historical pitching projection data from Fangraphs, and pitching season data from Baseball Reference. Along the way, I'll introduce the Python tools I used, and offer a few insights about scraping in general. Let's begin.

### Scraping Fangraphs

Let's import some relevant python libraries. I'm going to use the requests library to send HTTP requests. This is the part of webscraping that actually interacts with the website and brings data back. I'll use BeautifulSoup to parse the HTML that comes back, so it's easier to get the data I care about. I'm also going to load pandas for putting the data into dataframes once I've collected it.


```{python}
from bs4 import BeautifulSoup
import requests
import pandas as pd
```

The first thing that we'll do is get the website source, parse it with BeautifulSoup, and see what that looks like.

```{python}
response = requests.get('http://www.fangraphs.com/blogs/2017-zips-projections-baltimore-orioles')
orioles_soup = BeautifulSoup(response.text, 'html.parser')

result = orioles_soup.prettify().splitlines()
print(result[:20])
```

It looks like html, but boy is it a mess! I've only printed the first 20 lines here, but it can be very overwhelming to see all of this at once. To provide some guidance on what part of it we actually want to scrape, I'm going to use my browser to identify the table of interest, and inspect the page so I can narrow down where it is. I'm particularly interested in the pitching data, so I'm going to focus on the table that says "Counting Stats". 

![][1]

And if I right click the table with my mouse and click "Inspect", this pops up:

![][2]

I can navigate to the html responsible for that table using find() from Beautiful Soup, and passing in the title as text. It returns the text I'm looking for, so I'm confident that I've found the right place.

```{python}
orioles_soup.find(text = 'Pitchers, Counting Stats')
```

I'll use find.Next() to get what whatever comes after that tag, and keep only the text.

```{python}
crude_data = orioles_soup.find(text = 'Pitchers, Counting Stats').findNext().text
print(crude_data[:30])

```

It's bringing in the table header, which is what I want. However, it'll be hard to work with if every record is on its own line. I'll split the text so it's separated by carriage returns.

```{python}
crude_data = orioles_soup.find(text = 'Pitchers, Counting Stats').findNext().text.split("\n")
print(crude_data[:50])
```

Okay, this is starting to look closer to what I want, but there are two empty strings at the beginning of what should be each row. I'll write a quick list comprehension to get rid of them.

```{python}
data_cleaned = [i for i in crude_data if i != '']
print(data_cleaned[:50])
```

That looks much better. Now I'll combine everything into tables. There are a few different ways to do this, and this was the fastest solution I could come up with. Certainly there are other, more efficient methods. First I'll grab the headers from the data, and then remove them from the rest of the table data.

```{python}
headers = data_cleaned[:12]
data_no_headers = data_cleaned[12:]
```

I wrote a function to correctly grab the relevant values for each column.

```{python}
def extract_column(number, data, upper_limit, num_cols):
    name = []
    name_list = []
    for i in range(0, upper_limit):
        name.append(number + i*num_cols)
    for i in name:
        name_list.append(data[i])
    return name_list
```

This function takes a number (the index of the column I'll be populating), the name of the data, the upper_limit (determined by the length of the data divided by the number of columns in the table), and the number of columns. I'll use the Name column as an example.

```{python}
names = extract_column(0, data_no_headers, 44, 12)
print(names)
```

The function has returned all the player names. I can proceed across all of the columns and get the relevant values for each column by incrementing the index in the extract_column function (so we'll use 1 as an argument for Handedness or "T", 2 as an argument for Age, and so on).

Fangraphs has several different tables with varying number of columns, so when adapting this for other tables (for Pitcher Rates and Averages, or Pitcher Other, for instance), it's important to consider the number of columns when calculating the upper_limit.

```{python}
throws = extract_column(1, data_no_headers, 44, 12)
age = extract_column(2, data_no_headers, 44, 12)
games = extract_column(3, data_no_headers, 44, 12)
games_started = extract_column(4, data_no_headers, 44, 12)
innings_pitched = extract_column(5, data_no_headers, 44, 12)
strikeouts = extract_column(6, data_no_headers, 44, 12)
walks = extract_column(7, data_no_headers, 44, 12)
homeruns = extract_column(8, data_no_headers, 44, 12)
hits = extract_column(9, data_no_headers, 44, 12)
runs = extract_column(10, data_no_headers, 44, 12)
earned_runs = extract_column(11, data_no_headers, 44, 12)
```

So now I have several lists for each column. I will zip them all together into a pandas DataFrame.

```{python}
pitch_data = pd.DataFrame(list(zip(names,
                                   throws,
                                   age,
                                   games,
                                   games_started,
                                   innings_pitched,
                                   strikeouts,
                                   walks,
                                   homeruns,
                                   hits,
                                   runs,
                                   earned_runs)),
                         columns = headers)
```

I'm going to print out the first 10 rows, just to make sure this is what I want.

```{python}
pitch_data.head(10)
```

Excellent! This is exactly what I want.

This example can be used to scrape Fangraphs projection data for all teams, for multiple years. Scraping 2017 and 2018 data is easy with this method, but 2019 data looks different on the page, so the code should be modified accordingly. These adjustments are further described in my [github repo](https://github.com/angelinepro/baseball_scraping).

### Scraping Baseball Reference

This site requires a different approach. It renders pages dynamically, so some tables may only populate once a user scrolls to a certain part of a page. If I attempt to scrape the page without some kind of action that makes the table appear, I won't be able to find what I'm looking for in the page source. To fix this problem, I'll be supplementing my previous approach with Selenium, which "pretends" to be a user whose actions I can control, and can perform the scrolls and clicks on my behalf, making it possible for me to automate the process.

To start with, I installed ChromeDriver, a webdriver that opens the pages for me when I run commands in Selenium. Then I did the usual imports as above, as well as importing Selenium, and a module called time, which lets me pause web activity for some specified period of time, allowing the website to load.

```{python}
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time, os

chromedriver = "/Applications/chromedriver" 
os.environ["webdriver.chrome.driver"] = chromedriver
```

Now I'll start exploring the Baseball Reference page using chromedriver so I can scrape it. This will open a new browser window that navigates to the page. After the window opens, I'll sleep this for 5 seconds to give it time to fully load.

```{python}
driver = webdriver.Chrome(chromedriver)
driver.get('https://www.baseball-reference.com/leagues/MLB/2016-starter-pitching.shtml')
time.sleep(5)
```

![][3]

The table that I care about is the "Player Starting Pitching" table halfway down the page, and it doesn't show until I scroll down to it. I will have chromedriver scroll down to the 1500 pixel mark for me, and then wait a little while for that part to render. 

```{python}
driver.execute_script('window.scrollTo(0, 1500);')
time.sleep(5)
```


![][4]

There is an option to display this table as a CSV hidden in the menus, so I'm going to get chromeviewer to do it for me. I want it to click on where it says "Share & more". The location of this link in the source can be found by right-clicking and inspecting the source to find the relevant code. This is similar to the approach above for finding the part of the Fangraphs page with the relevant data. 

![][5]

Xpaths are another way to navigate around the page, and I thought it might be more straightforward to use them to find the link to click, so I right-clicked on the element and copied the xpath from there. 

![][6]

Now I can paste the copied xpath directly into the function find_element_by_xpath, and have chromedriver click on the link. I added another five seconds of sleep, again, just to give it time to load.

```{python}
driver.find_element_by_xpath('//*[@id="all_players_starter_pitching"]/div[1]/div/ul/li[1]').click()
time.sleep(5)
```

Using the same method, I can have it then click directly on the link to "Get table as CSV".

![][7]

```{python}
driver.find_element_by_xpath('//*[@id="all_players_starter_pitching"]/div[1]/div/ul/li[1]/div/ul/li[4]/button').click()
```

Now it's displayed as csv!

![][8]

So now that the page is showing what I want to scrape, I'll grab the source and parse it using Beautiful Soup, as I did for the Fangraphs page. 

```{python}
soup = BeautifulSoup(driver.page_source, 'html.parser')
soup.prettify().splitlines()[:20]
```

I'll navigate to the table and return what comes next. Using inspect, it looks like I want to find a 'pre' tag with the id 'csv_players_starter_pitching'. 

![][9]

```{python}
crude = soup.find('pre', id = 'csv_players_starter_pitching')
```

I'll split by newline to make the data easier to work with.

```{python}
crude_text = crude.text.split("\n")
crude_text[:10]
```

This resembles the raw table data from Fangraphs before processing it all into a proper table. I'll clean it all up, starting by removing empty strings, then splitting each line by commas, setting the first row as the header, removing it from the rest of the data, and finally converting it all into a pandas dataframe with the headers and index set appropriately.

```{python}
bbrefdata_cleaned = [i for i in crude_text if i != ''] 
bbrefdata_cleaned_newline = [i.split(',') for i in bbrefdata_cleaned] 
bbrefdata_headers = bbrefdata_cleaned_newline[0:1][0] 
bbrefdata_cleaned_newline2 = bbrefdata_cleaned_newline[1:] 
season_data = pd.DataFrame(bbrefdata_cleaned_newline2, columns = bbrefdata_headers).set_index('Rk')
```

Again, I'm going to print out the first 10 rows, just to make sure this is what I want.

```{python}
season_data.head(10)
```

Perfect! It's all there, and now I can analyze this dataframe using pandas. From here, I can save it as csv, so I don't need to re-scrape the data, using the to_csv() function. Now that I'm all done with scraping and have my data, I'll close the driver. 

```{python}
driver.close()
```

This example for scraping Baseball Reference can be used to scrape data for multiple years, using different tables. It may be too much work to use Selenium if you simply want the csv data for one year and one table, since it's easy enough as a human to do all the clicking and copy the CSV to a text file. The real benefit of using Selenium comes from needing to scrape multiple years of data, or multiple tables across different years. Here, automating the process and writing a function to do it can really save time.

### Wrap Up

I wrote a few functions that employ the principles described, that can be used to scrape the relevant pages on Fangraphs and Baseball Reference. These functions can be found on my [github repo](https://github.com/angelinepro/baseball_scraping). A word of caution about webscraping: these methods work so long as the webpage remains the same. Once the webpage is updated, some of the specific locations for the text of interest may also need to be updated, and the functions may need to be updated. 

Thanks for reading! I'll be continuing to post about what I'm learning while at Metis, so stay tuned.

[1]: /img/scraping/counting_stats_table.png
[2]: /img/scraping/counting_stats_inspect2.png
[3]: /img/scraping/bbref_start_pitch.png
[4]: /img/scraping/starting_pitch_table.png
[5]: /img/scraping/bbref_first_menu.png
[6]: /img/scraping/bbref_copy_xpath.png
[7]: /img/scraping/bbref_csv_table.png
[8]: /img/scraping/starting_as_csv.png
[9]: /img/scraping/csv_players_starter_pitching.png